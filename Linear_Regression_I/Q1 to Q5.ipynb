{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from autograd import grad\n",
    "import autograd.numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q2 Fitting linear model using scikit-learn. \n",
    "X = np.array([[1, 2], [2, 4], [3, 6], [4, 8]])\n",
    "y = np.array([2,3,4,5])\n",
    "reg = LinearRegression()\n",
    "reg.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(test_x,test_y,lm):\n",
    "    prediction = lm.predict(test_x)\n",
    "    error = mean_squared_error(prediction,test_y)\n",
    "    return error**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_data(dataset):\n",
    "    attr = dataset.columns\n",
    "    train = []\n",
    "    test = []\n",
    "    for index,row in dataset.iterrows():\n",
    "        if(index%3==0):\n",
    "            test.append(row)\n",
    "        else: \n",
    "            train.append(row)\n",
    "    return pd.DataFrame(train),pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.856774967008576"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q3(a) Real estate problem using sklearn.\n",
    "\n",
    "real_estate = pd.read_excel('real_estate.xlsx')\n",
    "train,test = divide_data(real_estate)\n",
    "features = train.columns\n",
    "input_features = features[:-1]\n",
    "target = features[-1]\n",
    "X = train[input_features]\n",
    "Y = train[target]\n",
    "X_test = test[input_features]\n",
    "Y_test = test[target]\n",
    "\n",
    "#Q3(a) The error evaluated on the test set.\n",
    "lm = LinearRegression()\n",
    "lm.fit(X,Y)\n",
    "score(X_test,Y_test,lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum coefficient is  158.16876217973874\n",
      "The maximum coefficient variable is  X6 longitude\n"
     ]
    }
   ],
   "source": [
    "#Q3(b)The highest  coefficient features\n",
    "maxi = 0\n",
    "loc = 0\n",
    "for i in range(len(lm.coef_)):\n",
    "    if(lm.coef_[i]>maxi): \n",
    "        loc = i\n",
    "        maxi = lm.coef_[i]\n",
    "\n",
    "print(\"The maximum coefficient is \",maxi)\n",
    "print(\"The maximum coefficient variable is \",features[i])\n",
    "\n",
    "#It is not correct to compare the variable values as the values of variables are function of their units a\n",
    "#and general trend of values. \n",
    "\n",
    "#We currently cannot comment on the importance of different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum coefficient is  0.29957850556990173\n",
      "The maximum coefficient variable is  X6 longitude\n"
     ]
    }
   ],
   "source": [
    "#Q3(c)The standardization of data from 0 to 1 and its testing.\n",
    "normalized = pd.DataFrame(features)\n",
    "for f in features:\n",
    "    normalized[f] = real_estate[f]/max(real_estate[f])\n",
    "    \n",
    "train_n,test_n = divide_data(normalized)\n",
    "X_n = train_n[input_features]\n",
    "Y_n = train_n[target]\n",
    "lm.fit(X_n,Y_n)\n",
    "\n",
    "maxi = 0\n",
    "loc = 0\n",
    "for i in range(len(lm.coef_)):\n",
    "    if(lm.coef_[i]>maxi): \n",
    "        loc = i\n",
    "        maxi = lm.coef_[i]\n",
    "\n",
    "print(\"The maximum coefficient is \",maxi)\n",
    "print(\"The maximum coefficient variable is \",features[i])\n",
    " \n",
    "#This is now a robust measurement of the maximum instead of using only the feature\n",
    "#This is now the most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x19c122f0ef0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Q3(d) Finding the distribution of residuals. \n",
    "\n",
    "residual_array = []\n",
    "lm.fit(X,Y)\n",
    "for index, row  in real_estate.iterrows(): \n",
    "    predict = lm.predict([row[:-1]])\n",
    "    residual = row[-1] - predict[0]\n",
    "    residual_array.append(round(residual))\n",
    "    \n",
    "dic = Counter(residual_array)\n",
    "x = []\n",
    "y = []\n",
    "for i in dic.keys(): \n",
    "    x.append(i)\n",
    "    y.append(dic[i])\n",
    "    \n",
    "plt.scatter(x,y)\n",
    "#Discretizing the Data for finding residuals. \n",
    "#We choose discretization of -10 to 10 with 0.5 interval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_features(train,no):\n",
    "    cutoff_1 = int(0.7*len(train))\n",
    "    train_model = train[:cutoff_1]\n",
    "    validate = train[cutoff_1:]\n",
    "    \n",
    "    se = set(itertools.combinations(features[1:-1],no))\n",
    "    mini = 20000\n",
    "    best_f = 0\n",
    "    for s in se:\n",
    "        sel_features = list(s)\n",
    "        tr_x  = train_model[sel_features]\n",
    "        tr_y = train_model[target]\n",
    "        val_x = validate[sel_features]\n",
    "        val_y = validate[target]\n",
    "        lm.fit(tr_x,tr_y)\n",
    "        per = score(val_x,val_y,lm)\n",
    "        if(per<mini):\n",
    "            best_f = s\n",
    "            mini = per\n",
    "        \n",
    "    return best_f,mini\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentPytorchRegression(X,Y,alpha=0.1): \n",
    "    \n",
    "    X = np.insert(X,0,1,axis=1)\n",
    "    cols = len(X[0])\n",
    "    rows = len(X)\n",
    "    theta = np.array([np.zeros(cols)]).T\n",
    "    \n",
    "    theta = torch.from_numpy(theta).float()\n",
    "    theta.requires_grad_(True)\n",
    "    \n",
    "    Y = torch.from_numpy(Y).float()\n",
    "    X = torch.from_numpy(X).float() \n",
    "    \n",
    "    for i in range(1000):\n",
    "        \n",
    "        theta.requires_grad_(True)\n",
    "        \n",
    "        predicted = torch.matmul(X,theta)\n",
    "        \n",
    "        error_val = sum((Y-predicted)**2)\n",
    "        \n",
    "        error_val.backward() \n",
    "        \n",
    "        gradient = theta.grad\n",
    "        \n",
    "        theta.requires_grad_(False)\n",
    "        theta = theta - alpha * gradient\n",
    "\n",
    "    return theta.numpy().T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentAutogradRegression(alpha = 0.1):\n",
    "    \n",
    "    grad_cost = grad(auto_cost)\n",
    "    theta = np.zeros(len(X_auto[0]))\n",
    "    for i in range(10000): \n",
    "        gr = grad_cost(theta)\n",
    "        theta = theta-gr*alpha\n",
    "\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentRegression(X,y,alpha = 0.1):\n",
    "    \n",
    "    size_x,size_y = np.shape(X)\n",
    "    X_ = np.ones([size_x,size_y+1])\n",
    "    X_[:,1:] = X\n",
    "    theta = np.zeros(len(X_[0]))\n",
    "    for i in range(100): \n",
    "        gr = gradient(X_,y,theta)\n",
    "        theta = theta-gr*alpha\n",
    "\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy(train):\n",
    "    cutoff_1 = int(0.7*len(train))\n",
    "    train_model = train[:cutoff_1]\n",
    "    validate = train[cutoff_1:]\n",
    "    features = train.columns\n",
    "    greed_f = []\n",
    "    best_score = 100000\n",
    "    cur_score = 0\n",
    "    while True:\n",
    "        flag = 0\n",
    "        for i in range(1,len(features)-1):\n",
    "            if features[i] not in greed_f:\n",
    "                tr_x  = train_model[greed_f + [features[i]]]\n",
    "                tr_y = train_model[features[-1]]\n",
    "                val_x = validate[greed_f + [features[i]]]\n",
    "                val_y = validate[features[-1]]\n",
    "                lm.fit(tr_x,tr_y)\n",
    "                per = score(val_x,val_y,lm)\n",
    "                if(per<best_score):\n",
    "                    flag = 1\n",
    "                    best_f = greed_f+[features[i]]\n",
    "                    best_score = per\n",
    "        if(flag == 0):\n",
    "            break\n",
    "        else:\n",
    "            greed_f = best_f\n",
    "    return greed_f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exhaustive_search(train):\n",
    "    min_error = 100000\n",
    "    chosen = 0\n",
    "    for i in range(1,5):\n",
    "        \n",
    "        opt_features,error = find_optimal_features(train,i)\n",
    "        if(error<min_error): \n",
    "            chosen = opt_features\n",
    "            min_error = error\n",
    "    return list(chosen),min_error\n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score with selected features 10.29021968269323\n",
      "score with all features 9.856774967008576\n",
      "score greedy is obtained as 9.866645460029128\n"
     ]
    }
   ],
   "source": [
    "#Q3(e)(i) and (ii) Taking optimal features and finding its performance. \n",
    "\n",
    "real_estate = pd.read_excel('real_estate.xlsx')\n",
    "\n",
    "train, test = divide_data(real_estate)\n",
    "\n",
    "features = train.columns\n",
    "\n",
    "greed_f = greedy(train)\n",
    "\n",
    "opt_features,min_error = exhaustive_search(train)\n",
    "\n",
    "test_x = test[opt_features]\n",
    "train_x = train[opt_features]\n",
    "train_y = train[features[-1]]\n",
    "test_y = test[features[-1]]\n",
    "\n",
    "lm.fit(train_x,train_y)\n",
    "\n",
    "print(\"score with selected features\",score(test_x,test_y,lm))\n",
    "\n",
    "\n",
    "#Performance of entire model\n",
    "\n",
    "test_x = test[features[:-1]]\n",
    "train_x = train[features[:-1]]\n",
    "train_y = train[features[-1]]\n",
    "test_y = test[features[-1]]\n",
    "\n",
    "lm.fit(train_x,train_y)\n",
    "print(\"score with all features\",score(test_x,test_y,lm))\n",
    "\n",
    "\n",
    "#Performance with greedy features\n",
    "train_x = train[greed_f]\n",
    "train_y = train[features[-1]]\n",
    "test_x = test[greed_f]\n",
    "test_y = test[features[-1]]\n",
    "\n",
    "lm.fit(train_x,train_y)\n",
    "print(\"score greedy is obtained as\",score(test_x,test_y,lm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalEquationRegression(X,y): \n",
    "    size_x,size_y = np.shape(X)\n",
    "    X_ = np.ones([size_x,size_y+1])\n",
    "    X_[:,1:] = X\n",
    "    A = np.linalg.inv(np.dot(X_.T,X_))\n",
    "    B = np.dot(X_.T,y)\n",
    "    th = np.dot(A,B)\n",
    "    return th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X,y,theta): \n",
    "    rows = len(X)\n",
    "\n",
    "    grad = np.zeros(len(theta))\n",
    "    for j in range(len(theta)): \n",
    "        for i in range(len(X)):\n",
    "\n",
    "            grad[j]+=-2*X[i,j]*error(X[i],y[i],theta)\n",
    "    return 1.0*grad/rows\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_cost(theta):\n",
    "    pred = np.dot(X_auto, theta)\n",
    "    return np.sqrt(((pred - Y_auto) ** 2).mean(axis=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(x,y,theta):\n",
    "    \n",
    "    err = y - sum(x*theta)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset(theta,X,Y):\n",
    "    size_x,size_y = np.shape(X)\n",
    "    X_t = np.ones([size_x,size_y+1])\n",
    "    X_t[:,1:] = X\n",
    "    error = 0.\n",
    "    for i in range(len(X_t)): \n",
    "        row_i = X_t[i]\n",
    "        dot =  np.dot(row_i,theta)\n",
    "        error+= (Y[i]-dot)**2\n",
    "    \n",
    "    return (1.0*error/size_x)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Equation  [4. 2.]\n",
      "Gradient Descent  [3.76333053 2.05234682]\n",
      "AutoGrad Regression  [3.97758245 1.8986462 ]\n",
      "pytorch regression  [3.9999928 2.0000017]\n"
     ]
    }
   ],
   "source": [
    "#Q4(a),(b),(c) and (d) The optimal paramter list by normal equation\n",
    "X = np.array([[1,3,6]]).T\n",
    "Y = np.array([6,10,16]).T\n",
    "t1 = normalEquationRegression(X,Y)\n",
    "\n",
    "\n",
    "#Q4(b)Applying gradient Descent regression.\n",
    "t2 = gradientDescentRegression(X,Y,0.05)\n",
    "\n",
    "#Q4(c)Applying Autograd regression scheme\n",
    "size_x,size_y = np.shape(X)\n",
    "X_auto = np.ones([size_x,size_y+1])\n",
    "X_auto[:,1:] = X\n",
    "Y_auto = Y\n",
    "t3 = gradientDescentAutogradRegression(0.05)\n",
    "\n",
    "#Q4(d)The pytorch regression scheme.\n",
    "\n",
    "X = np.array([[1,3,6]]).T\n",
    "Y = np.array([[6,10,16]]).T\n",
    "t4 = gradientDescentPytorchRegression(X,Y,alpha=0.01)\n",
    "\n",
    "print(\"Normal Equation \", t1)\n",
    "print(\"Gradient Descent \", t2)\n",
    "print(\"AutoGrad Regression \", t3)\n",
    "print(\"pytorch regression \",t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error obtained using normal Regression is  9.856774936956427\n",
      "The error obtained using Gradient Descent is  11.25784001214205\n",
      "The error obtained using Autograd linear Regression is  11.158562332904138\n",
      "The error obtained using Pytorch Scheme is  11.217159733055436\n"
     ]
    }
   ],
   "source": [
    "#Q4(e)Illustrating the above on the real estate dataset.\n",
    "file = pd.read_excel('real_estate.xlsx')\n",
    "train, test = divide_data(file)\n",
    "X_train = train[train.columns[:-1]]\n",
    "Y_train = train[train.columns[-1]]\n",
    "X_test = test[test.columns[:-1]]\n",
    "Y_test = test[test.columns[-1]]\n",
    "\n",
    "th = normalEquationRegression(X_train,Y_train)\n",
    "X_test = np.array(X_test.values)\n",
    "Y_test = np.array(Y_test.values)\n",
    "error1  = test_dataset(th,X_test,Y_test)\n",
    "\n",
    "#Q4(e)Illustrating the above on the real estate dataset with gradient descent.\n",
    "\n",
    "X_train = np.array(X_train.values)\n",
    "Y_train = np.array(Y_train.values) \n",
    "theta_opt = gradientDescentRegression(X_train,Y_train,0.0000001)\n",
    "error2 = test_dataset(theta_opt,X_test,Y_test)\n",
    "\n",
    "size_x,size_y = np.shape(X_train)\n",
    "X_auto = np.ones([size_x,size_y+1])\n",
    "X_auto[:,1:] = X_train\n",
    "Y_auto = Y_train\n",
    "theta_opt = gradientDescentAutogradRegression(0.000001)\n",
    "\n",
    "size_x,size_y = np.shape(X_train)\n",
    "X_p = X_train\n",
    "error3 = test_dataset(theta_opt,X_test,Y_test)\n",
    "\n",
    "X_py = X_train\n",
    "Y_py = np.array([Y_train]).T\n",
    "theta_opt = gradientDescentPytorchRegression(X_py,Y_py,0.0000000005)\n",
    "error4 = test_dataset(theta_opt,X_test,Y_test)\n",
    "\n",
    "print(\"The error obtained using normal Regression is \",error1)\n",
    "print(\"The error obtained using Gradient Descent is \", error2)\n",
    "print(\"The error obtained using Autograd linear Regression is \",error3)\n",
    "print(\"The error obtained using Pytorch Scheme is \",error4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.82345707e+03 -5.75536517e+04  3.57964583e+03  7.53760679e+02\n",
      " -9.47499055e+01  3.65204554e+00]\n",
      "[ 5.29201684e+04 -1.26089185e+05  2.78067073e+04 -2.48850506e+03\n",
      "  8.78523715e+01]\n"
     ]
    }
   ],
   "source": [
    "#Q5 Coefficients in case of 4th degree. \n",
    "\n",
    "x = np.arange(0, 20.1, 0.1)\n",
    "np.random.seed(0)\n",
    "y = 1*x**5 + 3*x**4 - 100*x**3 + 8*x**2 -300*x - 1e5 + np.random.randn(len(x))*1e5\n",
    "\n",
    "X1 = np.column_stack((x,x**2,x**3,x**4,x**5))\n",
    "theta = normalEquationRegression(X1,y)\n",
    "print(theta)\n",
    "\n",
    "#Q5 Coefficients in case of 5 degree\n",
    "X2 = np.column_stack((x,x**2,x**3,x**4))\n",
    "theta = normalEquationRegression(X2,y)\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
