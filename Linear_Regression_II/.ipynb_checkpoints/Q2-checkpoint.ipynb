{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(X,Y,alpha,num):\n",
    "    theta = np.zeros(len(X[0]))\n",
    "    \n",
    "    for i in range(num):\n",
    "        ch = random.randint(0,len(X)-1)\n",
    "        x = X[ch]\n",
    "        y = Y[ch] \n",
    "        theta = theta - alpha*gradient(x,y,theta)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_theta(X,Y,theta,ch): \n",
    "    rows = len(X)\n",
    "    coef = 0.\n",
    "    theta[ch] = 0.\n",
    "    num = 0.\n",
    "    den = 0.\n",
    "    for i in range(len(X)):\n",
    "        den += 2.*X[i,ch]**2\n",
    "        num += 2.*X[i,ch]*error(X[i],y[i],theta)\n",
    "    theta[ch] = -1.0*num/den\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lasso(X,Y,theta,ch):\n",
    "    rows = len(X)\n",
    "    coef = 0.\n",
    "    theta[ch] = 0.\n",
    "    zj = 0.\n",
    "    pj = 0.\n",
    "    for i in range(len(X)):\n",
    "        zj += X[i,ch]**2\n",
    "        pj += -X[i,ch]*error(X[i],y[i],theta)\n",
    "    return zj,pj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_lasso(theta):\n",
    "    cost = np.dot(X_auto,theta)\n",
    "    return cost\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso(X,Y,lamb,num): \n",
    "    theta = np.zeros(len(X[0]))\n",
    "    for i in range(num):\n",
    "        ch = random.randint(0,len(X[0])-1)\n",
    "        zj,pj = update_lasso(X,Y,theta,ch)\n",
    "        if(pj>lamb/2.): \n",
    "            theta[ch] = 1.0*(pj-lamb/2.)/zj\n",
    "        elif(pj<-lamb/2.): \n",
    "            theta[ch] = 1.0*(pj+lamb/2.)/zj\n",
    "        else: \n",
    "            theta[ch] = 0\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalEquationRidgeRegression(X,y,d): \n",
    "    rows,cols = np.shape(X)\n",
    "    A = np.linalg.inv(np.dot(X.T,X)+d*np.identity(cols))\n",
    "    B = np.dot(X.T,y)\n",
    "    theta_opt = np.dot(A,B)\n",
    "    return theta_opt\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinateDescentRegression(X,Y,num): \n",
    "    theta = np.zeros(len(X[0]))\n",
    "    \n",
    "    for i in range(num):\n",
    "        ch = random.randint(0,len(X[0])-1)\n",
    "        \n",
    "        theta = update_theta(X,Y,theta,ch)\n",
    "        \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(theta):\n",
    "    pred = np.dot(X_auto, theta)\n",
    "    return np.sqrt(((pred - Y_auto) ** 2).mean(axis=None)) + lamb*sum(np.abs(theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(x,y,theta):\n",
    "    err = np.dot(x,theta)-y\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x,y,theta): \n",
    "    \n",
    "    grad = np.zeros(len(theta))\n",
    "    err = error(x,y,theta)\n",
    "    for j in range(len(theta)):\n",
    "        grad[j] = 2*x[j]*err\n",
    "    return grad\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1,1],[1,3],[1,6]])\n",
    "y = np.array([6,10,16])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the coefficents learnt using ridge regression is [3.99956848 2.00008946]\n"
     ]
    }
   ],
   "source": [
    "#Q2(a) Applying normal equation regression on the dataset\n",
    "print(\"the coefficents learnt using ridge regression is\",normalEquationRidgeRegression(X,y,0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the learnt coefficient using Coordinate Descent is [4. 2.]\n"
     ]
    }
   ],
   "source": [
    "#Q2(b) Applying coordinate Descent regression\n",
    "print(\"the learnt coefficient using Coordinate Descent is\",coordinateDescentRegression(X,y,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the learnt coefficient using SGD is [3.95228453 2.00928597]\n"
     ]
    }
   ],
   "source": [
    "#Q2(c)lasso regression.\n",
    "print(\"the learnt coefficient using Lasso is\",lasso(X,y,0.1,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the learnt coefficient using SGD is [4. 2.]\n"
     ]
    }
   ],
   "source": [
    "#Q2(d) Applying stochastic gradient descent \n",
    "print(\"the learnt coefficient using SGD is\",sgd(X,y,0.01,10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the learnt coefficient using autograd regression is [4.00661368 2.03377523]\n"
     ]
    }
   ],
   "source": [
    "#Q2(e)Using Autograd to compute the same.\n",
    "X_auto = np.array([[1,1],[1,3],[1,6]])\n",
    "Y_auto = np.array([6,10,16])\n",
    "\n",
    "grad_cost = grad(cost)\n",
    "theta = np.zeros(len(X_auto[0]))\n",
    "alpha = 0.01\n",
    "lamb = 0.01\n",
    "for i in range(1000):\n",
    "    gr = grad_cost(theta)\n",
    "    theta = theta - alpha*gr\n",
    "    \n",
    "print(\"the learnt coefficient using autograd regression is\",theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
